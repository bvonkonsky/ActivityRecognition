---
title: "Qualitative Activity Classification"
author: "Brian von Konsky"
date: "18 September 2014"
output: html_document
---
#### Summary

A machine learning approach is presented to qualitatively classifying activity, based on data from
the [Weight Lifting Exercise Dataset](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) 
develop by [Velloso et al. (2013)](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf).
Their data were generated by attaching sensors to the body of experienced weight lifters and measuring attributes such as the orientation of the arm, forearm, pelvis, and a light weight held in the hand.  These attributes were measured for weight lifters performing the [Biceps Curl](http://en.wikipedia.org/wiki/Biceps_curl) exercise correctly, and for four additional situations in which subjects were asked to demonstrate specific attributes of incorrect form.  These five categories were labelled A through E in the data.

The method reported in the current project trains a [Random Forest](http://en.wikipedia.org/wiki/Random_forest) classifier to differentiate between each of the five categories.  The classifier was trained on a subset of the original data. Training using this approach resulted in a low out-of-bag (OOB) error rate of less than 1%, with a similarly low classification error rate across each of the five categories. The out-of-sample error was measured against a test set not seen during training, and was found to have an overall accuracy in excess of 99%.

#### Getting and Cleaning Data

Subsets of the [Weight Lifting Exercise Dataset](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) were manually downloaded from the website for the [Practical Machine Learning](https://www.coursera.org/course/predmachlearn) course from [Coursera](https://www.coursera.org/). These were stored in a directory called "data".  Note that the full dataset can be downloaded from the [Weight Lifting Exercise Dataset](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) web site.

Manual inspection of the data identified columns that were empty or contained NA values, or other information identifying test subjects and timestamps associated with each observation and not related to form. These columns were automatically discarded from the datasets. [R](http://www.r-project.org/) code used to read the data, discard unnecessary columns, and store the results is shown below.


```{r data}
# Returan the data frame used in this analysis
getData <- function(filename) {
  ## Set file names
  directory <- "data"

  # Set OS independentn paths to the files
  path.data<- file.path(directory, filename)
  
  # Read the data
  df <- read.csv(path.data, header=TRUE, stringsAsFactors=TRUE)
  
  # Clean the data
  keepCols <- c(8:11,37:49,60:68, 84:86,102, 113:124, 140, 151:159,160)
  df<- df[,keepCols]
  
  # Return the data
  return(df)
}

data.cleaned <- getData("pml-training.csv")
examination  <- getData("pml-testing.csv")
```

The data frame called "examination" was not used to train the system or measure out-of-sample accuracy. It was used by the system to predict the correct category, which was assessed by the Coursera grading system. The solution was awarded 20/20 marks for accurately predicting the category for the 20 cases in the examination dataset.

#### Data Partitioning

```{r libraries, echo=FALSE, results='hide'}
# Load libraries required by other packages, but hide this from Knitr output
library(lattice)
library(ggplot2)
library(e1071)
```

The cleaned dataset was partitioned into training and testing data sets. The training set constituted 75% of the data, and the testing set constituted the remaining 25%.  The training set was used to the train the system using the Random Forest method and measure the out-of-bag error to estimate the generalised error associated with unseen samples. The test set was subsequently used to measure the out-of-sample error associated with that particular set and further validate the predictive accuracy of the model. 

The code used to partition the data is shown below.

```{r Partition}
library(caret)
set.seed(1234)
inTraining    <- createDataPartition(data.cleaned$classe, p = 0.75, list = FALSE)
modelTraining <- data.cleaned[inTraining, ]
modelTesting  <- data.cleaned[-inTraining,]
```

#### Training

A random forest model was created using the randomForest() function from the [randomForest](http://cran.r-project.org/web/packages/randomForest/index.html) package.  This was used instead of the equivalent [train](http://www.inside-r.org/packages/cran/caret/docs/train) function from the [caret](http://cran.r-project.org/web/packages/caret/index.html) package with the method set to "rf". This is because early experiments showed that the randomForest package was better optimized to returned results more quickly than the caret package equivalent.


```{r Training}
library(randomForest)

# Use random forests to create a model
modelFit<- randomForest(classe ~ ., data=modelTraining)
```

The [Random Forest](http://en.wikipedia.org/wiki/Random_forest)  approach is an ensemble method that builds multiple [Classification And Regression Trees (CART)](http://en.wikipedia.org/wiki/CART) and outputs the most prevalent classification at the end of each iteration.  Each tree in the "random forest" is grown by the randomization of features when nodes are split using a different bootstrap sample.  This [eliminates the need](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr) for an explicit extra cross-validation step to estimate out-of-sample error.

The [Confusion Matrix](en.wikipedia.org/wiki/Confusion_matrix) resulting from applying this approach is shown below.  It shows that the out-of-bag (OOB) error arising from building the model using the [Random Forest](http://en.wikipedia.org/wiki/Random_forest#History) algorithm on the training set is less than 1%, as is the classification error for each of the five categories. The out-of-bag error can be used as an estimate of [Generalization Error](http://en.wikipedia.org/wiki/Generalization_error) associated with unseen data.


```{r}
modelFit
```

#### Out of Sample Prediction

The low out-of-bag error was further validated by predicting outcomes associated with data unseen by the training system.

```{r Prediction}
# check model with the partition set aside for testing
prediction <- predict(modelFit, modelTesting)
```

As shown below, the out-of-sample predictions show the model to be highly effective at classifying the data. For example, Category A (Correct Form) was correctly predicted 13395 times, and incorrectly predicted as Category B only once.


```{r confusionmatrix}
cm <- confusionMatrix(prediction, modelTesting$classe)
cm$table
```

Moreover, out-of-sample predictions resulted in 99% overall accuracy. This result is generally consistent with the low out-of-bag error estimates associated with building the original model.

```{r}
cm$overall[1]
```

Additionally, the out-of-sample results show high [Sensitivity and Specificity](http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity), and [Positive  and Negative Predictive Value](http://en.wikipedia.org/wiki/Positive_and_negative_predictive_values#Negative_predictive_value). 

```{r byclass}
cm$byClass
```

#### Conclusions

The [Random Forest](http://en.wikipedia.org/wiki/Random_forest) model was seen to produce a highly accurate classifier for the [Biceps Curl](http://en.wikipedia.org/wiki/Biceps_curl) activity in [Weight Lifting Classifier Dataset](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises). It is indicated that the [Random Forest](http://en.wikipedia.org/wiki/Random_forest) approach should be considered in future research to qualitatively classify human activity.

#### References

Biceps Curl, accessed 20 Sept 2014, [http://en.wikipedia.org/wiki/Biceps_curl](http://en.wikipedia.org/wiki/Biceps_curl)

Confusion Matrix, accessed 20 Sept 2014, [http://en.wikipedia.org/wiki/Confusion_matrix](en.wikipedia.org/wiki/Confusion_matrix)

Generalization Error, accessed 20 Sept 2014, [http://en.wikipedia.org/wiki/Generalization_error](http://en.wikipedia.org/wiki/Generalization_error)

Negative Predictive Value, accessed 20 Sept 2014, [http://en.wikipedia.org/wiki/Positive_and_negative_predictive_values#Negative_predictive_value](http://en.wikipedia.org/wiki/Positive_and_negative_predictive_values#Negative_predictive_value)

Positive and Negative Predictive Value, accessed 20 Sept 2014, [http://en.wikipedia.org/wiki/Positive_and_negative_predictive_values](http://en.wikipedia.org/wiki/Positive_and_negative_predictive_values)

R, accessed 20 Sept 2014, [http://www.r-project.org/](http://www.r-project.org/)
Random Forest, accessed 20 Sept 2014, [http://en.wikipedia.org/wiki/Random_forest](http://en.wikipedia.org/wiki/Random_forest)

Sensitivity and Specificity, accessed 20 Sept 2014, [http://en.wikipedia.org/wiki/Sensitivity_and_specificity](http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity)

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Weight Lifting Exercises Dataset, accessed 20 Sept 2014, [http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises)
